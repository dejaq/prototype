# V2 
A collection of new features and optimizations that are considered but may be or not implemented
## Web UI & Client
The web UI will be used to access all the topics in a cluster
See the state of the cluster (what brokers and clients exists)
Get basic metrics (connect to metrics endpoints)
Be a client (basic commands, producer and consumer)
Run custom count queries (for range and status of messages)

The project will be a separate project (similar with phpmyadmin, kibana)
The web UI will have a server side component that will do all the communication with the broker.
For gRPC calls see the gRPC-Web project
## The Broker - Load balancing v2
Version2 (dynamic weighted):
Any consumer can specify a starting weight that will be dynamically adjusted from:
No of leases a consumer has now (sent vs released)
Automatic or manual pushback commands

## Storage - Body Storage - checksums v2

Disabled by default, each topic will have the ability to activate the “checksums” option. This option is best suited for topics that have many messages with the same body. 
The BodyID will be the murmur3 128b checksum of the content, this way when a new message is inserted with the same body, an insert operation will be avoided. 

Schema:
BodyID: sum
Body: []bytes
messageIds: []string //when this is empty we can remove the message from the storage.

The Murmur3 was chosen after a small benchmark and research: https://github.com/bgadrian/go-checksum-benchmarks
Change in the broker operations:
Insert a new message 
If the body is new - insert a new Body
If the checksum already exists - add the messageID to the messageIds list
Remove a message - remove it from the messagesIds list
Remove the body if the messagesIds list is empty

The USER has to be informed that the risk of a collision will exist, which will lead to a loss of data. User will NOT want to activate it if:
is afraid of collisions
Want to lower the CPU overhead for producers
knows for sure that the messages are all unique
does not care about the storage space.
The messages are small (<1kb)

To optimize the network traffic as well, the SDK will query the storage BEFORE inserting the new message if the body already exists. This will add a round-trip message and a new COMMAND in the protocol. 

## Storage - Body Storage - mutability
Allow the producers to update the body of their messages. We will have to do extensive research on how it will affect all the modules like dealer cache and preloading.
There will be some limits, especially on the time when this operation will be allowed:
Message has to be available or waiting 

Because the time of the upload is unknown and is not an atomic operation, most likely we will have to allow producers to take a lease on the message or invent a new concept, possible issues may appear otherwise
2 producers uploads different payloads for the same message at the same time
The message does not have a least at the start of the upload, but it may have at the end when is too late, the consumer has the old payload 

## Retries/# of processed TODO
Now the USER has no way to know how many times a message has been previously processed (or attempt). There is no special message property and the body is immutable, so the user cannot put a value like: “retries”.
## New Broker Role: consensus
We can embed the etcd/Raft or another consensus in the brokers, to eliminate the need of an etcd cluster for small/medium systems. Basically the synchronization service will have the option to be embedded.
## Time drifts (should we fix this?)
Find ways to mitigate timedrifts without entering a deprecation level, like keep a local offset of the drift? I think this is a rabbit hole but further investigation should be made.

## Java, JavaScript & Python clients 
Although we love Go we recognize the need of having an official multi-language SDK. Once the main features are working and we actually know what and how an SDK should look like we will extend the SDK beyond Go.
## Messages reply/response/result (not decided yet)
Let the consumer save another payload into a different storage messageID=>response. Because tasks can be reprocessed we have to allow Consumers the freedom to: Overwrite or not the previous result. 
The results has to be stored independently (messages can be removed but the response has to live until the producers reads them. We can (and should) only allow retrival by the Producers (group) and query by taskID (or get all responses). 
Another way would be to ease and encourage the consumers to be producers on a different topic and publish the responses as messages (on a response topic). Maybe even build this functionality in the SDK? This will avoid creating 2 types of messages. 

## New topic: cronmessages
Instead of a timeline with timestamps we could make a new kind of topic that supprts cronjob semantics of scheduling and the brokers will basically generate a copy of the message each time it triggers as they watch them.


## New topic: Priority Queue (not decided yet)
Because the consumers receive the oldest messages (to the left of the timeline) first we can treat the Timestamp as a priority. With more or less tweaks of the systems (like allowing creating messages in the past) we can achieve a very large-range of priority values. Consumers will receive the next batch with the lowest value priority (highest importance), and inserting an older value means it will have a higher chance to be served to the consumers. We can even hide the Timestamp from the USER and creating in the SDK nice functions that work with Priority values (eg: from 0-max int32/int64). 
Facebook/Twitter timelines are valid use cases for this kind of topic.

## Embedded broker
 Allow USER that have Go apps to embed the broker in their apps. It should be easily possible (at least now when the storage is separated). Being open source and package based it should work. We have to consider this while writing the implementation.



