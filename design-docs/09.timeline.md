# Topic: Timeline
Topics are unique in a cluster (by ID).
## Best use cases
### Scheduled actions
When an app needs time-triggered actions they are usually solved with a cronjob that acts like a tick, and checks if any of the event needs to be triggered. Using a TImeline your consumers will receive the messages at the right time, there is no need to check or poll. Setting the Timestamp for each message, the consumers will receive it only the current time precedes it. Examples:
Schedule a marketing campaign with personalized emails 
Set new user notifications (1, 3 and 7 day) at a user registration
Re Engagement messages at specific timestamps based on a user action (8h after a purchase)
Subscriptions invoice payment notifications
User customized alerts
Reminders
### Hybrid/complex queue system 
The timeline is not built to be used as a queue pipeline, but because of its characteristics can be used for complex distributed worker-processing type of systems. The consumers have the ability to move a message into the future (before or after finishing the processing), examples:
sampling/partial degradation of consumers: I cannot process this message right now, give it to me in a few minutes
Exponential retries on some messages: give the message later
Cancel the message: a user action or a message property invalidates the scheduled messages, and it can be removed by the producer or consumer

### Scheduled Jobs
If we treat messages like jobs, and consumers as workers we can use the timeline as follows:
Producer - publish a Job
Messages is a Job 
Consumer takes a lease on the message (starts the job)
Consumer extend the lease (I need more time to finish the job)
Consumer deletes the message (finish job)
Consumer may die/freeze - the job (message) will reach to a new consumer

### Time buffer between queues
If you have 2 topics and want to add time-delays you can put a Timeline between them. Example:
Kafka Producer creates messages 
Timeline reads from the topic and stores the messages for a few hours 
Timeline consumer receives the messages when the timeout occurs and reinserts them to another kafka topic
TODO improve this description

## Implementation

The messages are spread on a timeline based on its timestamps. Producers choose the initial position of the message on the timeline (in the future). Producers will have the absolute right on the message they generate for its entire lifespan (move, delete).

The broker will push all the available messages (that have the timestamp property in the past <= NOW) to a free and healthy consumer. The broker will also create a lease on behalf of the consumer and sends it.

The Lease Timeout of the consumer is actually moving the message in the future, basically acting as a visibility window/timeout processing feature. If the consumer does not Extend its Lease it means it died/gave up and the message will be served again to the consumers when the NOW reaches its Timestamp+Timeout.

Consumers will receive the lease (having a lock) and can start the processing. After its done the message should be deleted or returned to the timeline in the future with Release() (so others consumers pick it up later, again). A scenario can be just to relay/pushback the events: get the Lease, Release() it into the future without processing it (eg: the service is in a partial degradation state, give me this event later on). 
The broker will push the available messages to the consumers until they are deleted or moved into the future on the timeline. Basically if the consumers don’t do anything with the first messages the system will enter a livelock.

To avoid spikes (large amount of messages in a time interval) we suggest putting fewer messages and fan-out at the consumer level (eg put a message with multiple user emails).
## Timeline and time

Because the time is at the core of the System special considerations has to be taken. 

I think that the sync issue is best handled at an OS/server time level so the Client SDKs will read the computers time as often as it can (to not affect the performance). 
### Time drifts 

First of all to limit the synchronization issue between servers we can calculate/write most of the timestamp values in the broker. This way we limit the issue only at a small number of servers. 

Brokers can calculate an avg timestamps from the pings with all its connected SDKs. Any time-drift detected should be reported in the metrics.

At a later stage (after the System is more mature) we can address the issue that the OS cannot be trusted by reading the Time from a 3rd party more trusted source (let the user decide which one of course). More research has to be planned for this and see in which contexts this issue may appear more often (systems that cannot be updated? Servers with blocked ports like GovClouds? IoT devices?). 

Time drifts will make the system misbehave
At the Storage level
We do not use the Timestamp() from the storage, the queries will already have the timestamps populated
Drift may affect the replication process (we will see when we make that system)
At the Broker level 
In some cases a time drift will result to data loss (in a way or another)
Is the only generator of timestamps in the systems, this way we can control/mitigate the drift issue from one place
Using the metadata and ping we know the time difference between any 2 brokers that communicates
We can detect time drifts (relative to cluster) and we will alert the USER ASAP (with metrics and errors)
Mitigations:
The broker will enter a degradation state (see Broker chapter)
We can calculate the majority of the brokers Timestamp (offset) and apply that to all generated timestamps. Do we want to do that? with a bad implementation we will do more harm than good 
Operations and effects:
Producer Create() message with relative timestamp: the broker will put the message at a wrong timestamp - no data loss, inconvenience
Consumer Lease/Extend relative timestamp: the broker will save the wrong lease expiration, more leases can exist if the drift is in the past
Consumer Push: it will fetch and push the wrong messages if the drift is in the future
At the SDK level - our SDK and the User code are affected by the system time
Consumer Lease.IsExpired() will result in errors (past) or false positive (future)
Producer generate relative timestamps for Create, UpdateTTL (eg: Now+200): it will put the messages at the wrong time 
Consumer does relative timestamps for Extend/Release: will result in errors or wrong timestamps 
## Limits
Because of the mutable nature of the timeline (topic) these will never happen:
Replayability - the topic cannot be replayed because the messages keep moving on the timeline and get deleted
Balancing/Predictability - some timeframes may have more messages than others, although the Consumers will have the ability to “see the future load” and react if needed (scale)
Ordering - the natural ms granularity should be suffice to guarantee an app-level ordering. I consider that a real-world app cannot guarantee in a distributed system the ordering of incoming messages to be delivered in the same ns in a specific order.
## Delivery guarantees
Because of the nature of the system, producers usually want to pinpoint messages on the timeline, so they will not create spikes in a specific time interval. The Broker has to assure that the messages are saved in the storage, sacrificing performance and throughput to durability.

### Exactly-once semantics
This concept is tricky and not really applicable to our system as a hole, because the message can be reinserted in the queue by the consumers. But we will treat the case in which the producer will want to publish a message to be consumed only once (and removed afterwards).

### Publishing
In order to assure that a message exists only once in the system the USER has to work with the system, making the Create operation idempotent.
Natural IDs - if the USER can generate the same ID (eg: users email) even if it retries after a failure the system can guarantee the unicity of that message 
USERs store the messageID generated by the SDK at publishing attempt - and retries with it (see A) 

### Lease 

To assure that if the message is Leased only the consumer will delete the message while it has the lease.

First of all, the system can only guarantee that only 1 Lease exists on a message at a time. The Client has to make sure to remove the message while that lease is active to assure that no other consumer will ever receive that message. 

### Processing
To assure that the message is processed only once: is not in the scope of the project, it can be achieved in two ways
by treating the Processing and Deletion of the message in an atomic transaction. (if one is not possible rollback the other)
Make the processing idempotent (eg: after processing the Consumers save the result in a storage with the primaryKey the messageID. The consumer after getting a Lease can check if is already processed OR it doesnt save the result if one already exists. A storage like Redis or persistent with a TTL of 2 x LeaseTimeout should cover all cases). 

### USERs responsibilities

Things the user has to do to achieve exactly-once semantics
Make the processing idempotent
If the consumer restarts - it has to have the same ConsumerID
If the consumer restarts during a processing - it has to resume its processing (the SDK will give its lease if they are still valid)
The Lease timeout has to cover for a restart time too (make your leases last long enough so you crash during a process, recover, and have time to issue an Extend too) 

☑ - means the USER will benefit once-processed semantics despite the system failures
☒ - means there is a good chance that 2 consumers will have a Lease (if the TTL permits and failures occurs) and at least 2 Processing process started on a single message 

Network split - will be detected and the minority will not generate leases.
Broker fails
Sends the message to SDK ☑, but fails before the Lease: the lease will fail because the broker is down 
Fails during a lease command 
(the lease is saved in storage but did not reached to consumer ☑ (no processing started)
The lease is sent to consumer: 
Fails when the Extend
The consumer partially processed the message and has to stop - depending on the client logic it may affect it or not ☒
Fails at Delete
The consumer finish processing, sends the Delete command and no broker responds in time and the lease expired: ☒
Fails at Delete reply to consumer 
No other consumer will get a lease, even if the consumer doesn't know and The next Delete retry will be a noop ☑
TODO add others
Consumer fails 
During a lease
During the process the Extend fails - it has to stop processing at the end of the lease - ☒ 
 it finished to processes but the Extend/Delete commands do not reach the Broker ☒


### Producers check for processing

The producer cannot really 100% confirm that his message was processed eventually. The consumer, by achieving a lease confirms the system that the message was acquired. There is no way to communicate that a message was successfully processed (only acquired). 

If the producer relies on the fact that a message is removed from the Timeline by the consumer once it is processed should work in most cases. False-positive cases can appear when:
Network partition on the broker-persistent storage
Persistent storage data loss/outage

## The Lease 

A message can be leased to a consumer for a specific period of time (specified by the consumer). 
The lease is lend to a specific  consumer instance (process/thread). Because the processing of a message cannot be resumed in most cases and is done by a single process at a time, we linked the Lease to a specific process. 
When a consumer returns in the cluster (eg after a crash) will have the ability to refetch its active leases.

A lease is a STATE of a message and a property of a consumer. To determine if a lease exists and is valid we need the latest state of the message and the current values for its properties. Meaning that if we have stale data the Lease can be false-positive. 

How to determine if a lease exists  and its owner: 
message.LockConsumerID is empty - there is no lease
message.LockConsumerID is not empty but the message.Timestamp < NOW() - the lease expired 
message.LockConsumerID is not empty and message.Timestamp >= NOW() - the lease is active and the consumerID is in the message. 

While an active lease exists, not even the OWNER (producer) can delete the message from the timeline nor the system (if is expired). The LEASE is a contract  (the consumer has the absolute right over that message) so it cannot be broken.
	
## Message on a Timeline topic
The central piece of the system, the message has some metadata attached and a payload. Messages can exists anywhere on a timeline, their position is decided by the Timestamp field.

The structure of a message: 
 * ID - unique string across the topic. UUID or it  can be a KUID/sortableID that can also serve as the original creation/timestamp date (useful to determine if the message is a retry/was rescheduled/reused). 
 * Timestamp - the UTC milliseconds, position on the timeline
 * BodyID - the ID of the body (see Storage-body)
* ProducerGroupID - to allow a group of producer to update/delete a message
* LockConsumerID - (the consumer who has the lease, if any)
* _BucketID - populated by the broker at produce time, random chosen based on topic settings, to allow dynamic partitiong system to function
* _version - used for consistency and only-once delivery

The ID, BucketID, Body and ProducerGroupID are immutable for the message lifetime.

ID - can be provided by the Producer (naturalID), if is omitted the broker will generate one. If natural IDs are uses, the system WILL be prune to performance issues, more exactly the USER has to avoid making HotPartitions (many messages that exists in the same SHARD). It has to know and understand how the Storage partition system works.

Version - is used by the caching mechanism to ensure that the producer did not update the message while it was in transit for the consumer. Eg: Broker take MessageA from the storage and push it to the SDK with all the fields and version 1. The SDK exposes only its ID (at subscribe). The OWNER modifies the timestamp and the version is increased (2). SDK known timestamp reaches NOW and consumer tries to get a Lease on the messageID with version1. It will failed on the storage level because now it has version 2. The SDK will decide if if will refetch the Message and try the Lease one more time or quit (if the timestamp changed most likely it will be served by the broker to another consumer at that new timestamp). 

ProducerGroupID and LockConsumerID can also be natural identifiers or generated by the system.

A message has 3 types of states: 
Status (dynamically determined, available, waiting and processing)
Owned - (static and immutable) it is owned by the producer group (group of processes)
Leased - (action based, true or false) a consumer (specific process) has the temporarily total control over it

Based on its properties and the current NOW() timestamp, the status can be (the order which they are evaluated is important:)
Past messages (Timestamp <= NOW) = available status. They can consist of 
New messages (not seen by a consumer)
Lost messages (consumer had a lease but expired) (LockConsumerID not empty)
Released messages (consumer released them in the past and now they are reseen) 
//for now there is no way to distinct between New and Released messages
Future (timestamp > NOW)
Waiting status (new messages) (LockConsumerID is empty)
Processing status (it has a Lease) (LockConsumerID not empty)

Actions that can be made on the message, after the OWNER creates it. 

|                  	| Update Body 	| Read Body 	| Delete                                              	| Extend Lease 	| Release                                                	| CockroachDB 	|
|------------------	|-------------	|-----------	|-----------------------------------------------------	|--------------	|--------------------------------------------------------	|-------------	|
| Owner            	| no          	| yes       	| Yes (if is waiting)                                 	| no           	| no                                                     	| yes         	|
| Other producers  	| no          	| no        	| no                                                  	| no           	| no                                                     	| yes         	|
| Consumer w/lease 	| no          	| yes       	| yes                                                 	| yes          	| yes                                                    	| ???         	|
| Other consumers  	| no          	| no        	| no                                                  	| no           	| no                                                     	| ???         	|
| ????             	|             	|           	|                                                     	|              	|                                                        	|             	|
| Subscribe        	|             	| CTB       	| Must be an unsubscribed consumer.                   	| R            	| Error error                                            	|             	|
| Unsubscribe      	|             	| CTB       	| Must be subscribed                                  	|              	| Error error                                            	|             	|
| Info             	|             	| CTB       	| Fetch the latest metadata consensus from the broke. 	| R            	| Metadata metadata (see SyncrnizationService consensus) 	|             	|

Not all operations are available all the time. Each operation on a message has to be reverified by the storage (as a condition in the atomic operation), this will fix the most issues with Time drift and bad actors:
Create
The ID does not exists
Lease  (newTimestamp)
Status == available
Leased == false
newTimestamp > NOW+few ms
Extend lease  (lease, newTimestamp)
LockConsumerID == lease.consumerID
Timestamp > NOW (lease is active)
newTimestamp > NOW+few ms
Release (lease, newTImestamp)
LockConsumerID == lease.consumerID
Timestamp > NOW (lease is active)
newTimestamp > NOW+few ms
Delete (from OWNER)
Timestamp <= NOW OR (timestamp > NOW && LockConsumerID is empty (no lease))
Delete (from consumer)
LockConsumerID == lease.consumerID
Timestamp > NOW (lease is active)


For performance reasons the Delete will only mark it as deleted, and the actual storage removals will be made by various brokers in batches.

## Timeline storage

The mutations/lookups/queries that will be done (in order of importance) for the Timeline Topic:
INSERT messages ([]messages) map[msgID]error
SELECT message.ID BY BUCKETIDs ([]bucketIDs, limit, maxTimestamp) ([]messages, hasMore, error)
UPDATE timestamp BY MESSAGEIDs (map[msgID]newTimestamp])  map[msgID]error (for extend/release)
LOOKUP message by messageID (owner control, lease operations)
DELETE messages by messageID map[msgID]error
COUNT messages BY RANGE (spike detection/consumer scaling and metrics)
COUNT messages BY RANGE and LockConsumerID is empty (count processing status)
COUNT messages BY RANGE and LockConsumerID is not empty (count waiting status)
SELECT messages by LockConsumerID (when consumer restarts)
SELECT messages by ProducerOwnerID (ownership control)

Sharding key and ranges: message.bucketID, message.timestamp
Primary key: message.ID 
Secondary index: LockConsumerID
Secondary index: ProducerOwnerID

For the body of the message see Storage - Body
