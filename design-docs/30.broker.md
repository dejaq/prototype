# The Broker 

The main implementation will be written in Go, acting like an interface to the storage system, for both the producer and consumer side. It has to have the properties;
User Friendly (API)
Low latency 
Batch handling
Flexibility (scaling consumers up/down)
Give the consumer data so it can react (eg; spikes)
Monitoring friendly and precomputed stats
Extensible - Onion layers with Interfaces in between
Resiliency - put RECOVERs everywhere! 

Brokers can have one or more roles in a cluster.

(Not finished) Partial degradations
Time DRIFT: when broker system time is very different from the other brokers, SDKS and/or Syncronization service. It has to stop doing cleanup on databases (will delete the wrong data), stop writing to DB operations that involve localTimestamp, stop pushing messages (will push the wrong data)
When the storage keep returning errors (or last ops failed) - sample x% of operations, refuse others
When the other brokers disappear (most like is a network partition) 
When most of the Consumers returns errors or are busy (stop push messages) 
When most of a producer requests result in errors: throttle it

From the Cluster perspective, a broker can be:
Healthy
Degraded (from any reason)
Inactive (the server is not online, but should return sometime)




## Overseer (role)
Clients first has to connect to an overseer before handling or creating messages. It will provide commands for cluster management. 
Responsibilities:
Provision topics (create and delete)
Calculate weight/preference list for all topics (which broker is best to connect to)
Listen to producer & consumer management commands (create topic, fetch cluster info …)
Expose stats/metrics 
Keep a list of brokers and their metadata (ip, latency …)
Making sure a Topic is deleted (from storage, see #deleting a topic)

To calculate the weight/prefernce list it has to connect to brokers and fetch its metrics, this way it can auto-balance the load between traffic in real time, suggesting consumers and producers to switch to other brokers.
## Broker Selector
When a client becomes consumer or producer, the brokerSelector is responsible to choose a broker for it. 
It requires all the cluster metadata (from brokerWatcher), and applies a simple algorithm:
priority 1: broker that has tlistOfTOpicsAlreadyHandles the corresponding topics BUT has a maximum value of X (to avoid hotloading the broker). To optimize the resources, a broker is best to have 4 consumers on each topic but maximum of 7??
Priority 2: broker with the least number of tlistOfTopicsAlreadyHandles
Why is better then allow random brokers? To split the load and also provide better performance. 

Because of the Dynamic Partitioning of the topics, brokers has to fetch all the messages metadata from the storage, and decide which of them will go to the consumers that are connected to it. It would be inefficient to split a topic load too thin, for example:
A cluster, with 1 topic has 8 consumers and 4 brokers. If we split them between all the 4 brokers, all the brokers will have to read the same Database shard (for the current messages). If we group the consumers to only 2 brokers, there will be less network traffic and DB load. 
The same logic applies to Producers, but with a write load on the DB.

This list of weights/preferences will be calculated by the overseer and returned to each client, when requested.
## Creating a topic
Add to etcd - in special state 
Provision the storage 
After is finished clients can start making handshakes for it
## Deleting a topic
The deletion cannot happen in sync (because the topic may be very large), but also we have to delete it fast. Commands has to fail fast and also the user has to be able to create a new topic, with the same ID right after the deletion. 

Deletion from SynronizationService (etcd) - move the entry in a special etcd list
This will trigger a deletion from all brokers memory and terminate current clients
The storage delete query has to be sent and made sure the topic is not available any longer 
To cover the case in which the broker dies right after commits the deletion in etcd, and the storage command was not sent, we have to keep the topics in a special deleted list in etcd. Brokers has to check that list and make sure the entire deletion is finished. We need to choose a Master for each deleted topic entry to handle this. 
During this phase, a new topic CANNOT be created using the same ID
After all these operations finish successfully the broker can remove the “deleted topic entry” 
## Carrier (role)
While the Overseer is the manager, the carrier is the heavy-duty worker process. It handles all the messages traffic, and is the main consumer of the storage.

Responsibilities:
Act like a gateway to the storage 
Listen to producer & consumer messages commands (insert, release, delete …)
Push messages to consumers at the right time (single digit ms delay)
Load balance the messages between consumers 
Generate Leases for its consumers
Calculate the avg latency for each consumer (used by the push mechanics to reach on time)
 has to detect timedrifts and alert the clients. Broker can expose the timedrifts in a Metric.
Expose stats/metrics 

### Loader: Consumer load distribution (topic partition and push messages)
Part of a Carriers task is the responsibility to deliver messages in time to consumers. The component is called “Loader”
From the broker's perspective, each active Consumer is a topic partition. It has to decide which gets what message. Liveness/healthcheck and busy status has to be pulled/pushed from/to each consumer at a small interval. 

A simple choice would be to give a broker leadership on a topic. This approach would lead to a couple of downsides: 
Hot topics will load a broker, consumer commands has to be rerouted to the leader. The main advantage would be a simple distribution of messages between consumers (partitioning). 
We can avoid these downsides and build a more flexible system by using a deterministic dynamic partitioning system.

The order of the messages sent to multiple consumers cannot be guaranteed, but all the messages has to be processed.
TODO: decide, should we guarantee the Leases order sent to a specific consumer by its timestamp OR by its leaseExpirationTimestamp OR random order.

Based on the premises that:
We have a list of available messages (hopefully immutable/locked) to be distributed
There are one or more available consumers
Consumers can connect to any broker to subscribe to any topic (with preferences, see Driver/BrokerSelector)
Producers can create messages via any broker
The list of brokers and consumers is the same on all brokers (consensus protocol)
The weight/processing duration of all messages are equal

We presume that all clients have the same hardware capabilities, all of them are equal so we round robin the load), these restrictions will be removed in version2 of the broker.
Messages have the same processing time
Consumers have the same processing capabilities




#### Rebalancing, Partitions and time windows
To mitigate the performance degradation of the cluster during a rebalancing we have to work in time windows. 
In TimeWindow1 (T0), based on the current consumers we calculate the distributions of messages for T1. In T2 for T3 and so forth. 
New consumer: will only receive messages in the next window
Lost consumer: its available messages (not having a lease) will be consumed by the backups (see rack-aware), or in the next interval by other consumers. Its leased messages will be “naturally” be divided in the window they become available.

At a timeline level, all the messages are processed as they are placed on it (based on their timestamp value). To divide the work, messages will be split between consumers, so we cannot guarantee that the messages will be processed in order, but we can guarantee they will be served to consumers.

Basically, brokers will select all the available messages, Sorted by Timestamp ASC, with a LIMIT (for prefetching, in loader). This way T2 will contain:
T0...T2 available messages (if the timeline has lag)
T2 available messages

We treat all messages from a time window equal, meaning they should all be processed ASAP. All buckets are the same and will be processed in a random order.
#### Buckets and hashing
Each message will receive a random bucket at creation. This way we try to do an even distribution between storage shards and consumers. Bucket will be an uint16 number. This will also be the maximum number of consumers (one bucket cannot be divided to multiple consumers, but a consumer can have multiple buckets).
The algorithm will distribute series of buckets to each consumer, and the loader will fetch the messages from the respective buckets and sent to the producers.

#### Consensus
There is no direct communication between the brokers, all consumers are saved in the Synchronization (etcd)
Combined with the fact that we will implement a deterministic algorithm, each broker can computes each consumers buckets.
We do not need a fancy algorithm like Consistent hashing or Rendezvous Hashing because the messages do not have to reach the same consumer (if the consumer crashes scenario is solved by the lease), also the messages are transient (most likely they will be consumed only once in their lifetime). We just need a uniform distribution so we can use any hash function with a simple interval sharding.

#### Dealer: Simple ranges
The basic algorithm consists of evenly distributing all buckets to all active consumers. 
All brokers knows which of those consumers are connected to them (by ID), and fetch the SetOfConsumers. Ordered by IDs, consumers will receive a range of buckets, eg, for 3 consumers (A,B,C) and 2048 buckets
A: [0-682]
B: [683-1364]
C: [1365-2048]

Broker 3 has consumers A and C connected to it, the Loader will fetch from the database only the messages that have a BucketID between intervals:  [0-682] and [1365-2048]. The messages will be pushed to the consumers. 

This method will work on the following presumptions
All the brokers have the same metadata (SetOfconsumers)
The BucketID is randomly chosen (even load distribution)

#### Dealer: rack-aware 
This is a more complex algorithm, designed for large clusters distributed across K8S clusters and/or availability zones. The downside of “simple ranges” is that is susceptible to rebalancing actions. 
When a consumer goes down, the available messages that were assigned to it (range of buckets), will not reach a consumer until: 
Next time window
Consumer comes back online
This will cause the broker buffers to be invalidated, and it will hurt the cluster performance and accuracy. In a large cluster this is a matter of When not If consumers disappear. 

The client will have the obligation to provide each customer also a “RackID”, and preferable to distribute even number of consumers in each rack. 
The Loader will distribute all the buckets to each Racks, and engage consumers in a “race to process all of them”, using a Speculative execution pattern (consumers will have backups in all the other racks).

Eg: buckets [1,2,3,4,5,6,7,8,9,10,11,12], we have 2 racks (A, B) and each of them 2 consumers (A1, A2, B1, B2). 
The buckets (and their messages) will be split like this:
A1: [1,4), [10,13) (backup of B1)
A2: [4,8),   [7,10) (backup of B2)
B1: [7,10), [1,4)(backup of A1)
B2: [10,13) [4,8) (backup of A2)

If the entire cluster B goes down, consumers from rack A will continue their range and go through all the messages.
If nothing bad happens, all consumers will try to get a lease for the messages and it will fail, because they were already processed by the other consumers, eg:
The Loader will try to make a lease for the messages from the bucket 7 for A1. It will fail because 7 was already processed by B1, so the messages from the current time window are not available anymore
If some messages take more processing time or something happens to B1 and slows down the processing (lag), 9 and 11 will be processed by A1. 
Not all buckets will have messages all the time, so if the buckets 8 and 10 are empty, the B2 will process also 6, so both B2 and A2 will finish earlier. 

TODO: 
Find a way to sort/group consumers based on their IDs with minimal rebalancing side effects
Make the algorithm for unbalanced racks (different number of consumers per rack)

Complex example:
A1:
A2:
A3:
A4:
B1:
C1:
C2:
### LeaseDuration, TimeWindows and at-lease-once-delivery

Because we try to guarantee an “only-once” delivery, we cannot send the same messages to multiple consumers (rack-aware will try), the lease system will prevent it.
The client will have to tweak these settings to balance between a high throughput vs latency/accuracy
LeaseDuration (consumer setting): small value will help its messages to reach another consumer faster, after a failure, but it will also increase the chance of having “more-than-once” delivery if it cannot communicate to the broker after a processing (or the lease expires)
TimeWindows: (topic or broker setting?!!?): small value will help the messages reach other consumers faster in a failure (available messages), high value will allow brokers do more pre-fetching, more throughput but in the case of Consumers disappear, its messages will wait longer to get distributed to another consumer (rebalancing for next T) 


## Metrics 

The broker will export metrics for all of its inner features. The data model is compatible with the Prometheus metrics. Labels will be used to identify each topic.
Job: chrono_queue
Brokers will have the ability in the settings file to allow the USER to add custom tags that will be used to metrics. Eg: they can put the deployment stage.

Topic_commands_count {op=create|delete topic=name cluster=name}
Topic_commands_errors {op=create|delete topic=name cluster=name}

topic_Messages_count {op=create|delete|released topic=name cluster=name}
topic_messages_errors {op=create|delete|released topic=name cluster=name}
topic_Available_messages_count {topic=name cluster=name} //for consumer auto scaling
topic_Waiting_messages_count_bucket TODO histogram for the next 2-4 TimeSlices/Shard

topic_Leases_count {op=create|extend topic=name cluster=name}
topic_Leases_errors  {op=create|extend topic=name cluster=name}
topic_Leases_body_total_bytes {topic=name cluster=name} //detect large messages


This will help to detect any misc-issue not spotted in the other metrics, like issuse with Ping
Also messages and leases commands will have the sum of batch sizes, here they will have the commands
broker_commands_errors {command=ALL_commands_list  topic=name cluster=name} 
broker_commands_count {command=ALL_commands_list  topic=name cluster=name} 

broker_Active_timedrifts_count {target=storage|sync|broker|consumer|producer topic=name cluster=name}
broker_Active_timedrifts_seconds_avg  {target=storage|sync|broker|consumer|producer topic=name cluster=name}
broker_Degradation_state {type=TODO topic=name cluster=name} //gauge

storage_queries_count {op=create|update|delete type=range|lookup filter=all|available|waiting|processing topic=name cluster=name}
storage_queries_errors  {op=create|update|delete type=range|lookup filter=available|waiting|processing topic=name cluster=name}
storage_query_duration_ms {op=create|update|delete type=range|lookup filter=available|waiting|processing topic=name cluster=name}

Network_broker_bytes_total {direction=in|out partner=sync|storage|producer|consumer  topic=name cluster=name}
Network_broker_latency_ms  {partner=sync|storage|producer|consumer  topic=name cluster=name}
network_broker_tcp_connections_total
Networkbroker_active_channels_total {type=consumer|producer topic=name cluster=name} (gauge)
Networkbroker_active_channels_errors {type=consumer|producer topic=name cluster=name}
